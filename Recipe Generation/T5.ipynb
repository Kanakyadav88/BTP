{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11202324,"sourceType":"datasetVersion","datasetId":6994306}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" # t5_trainer.py\nimport pandas as pd\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:23.280816Z","iopub.execute_input":"2025-03-29T14:56:23.281117Z","iopub.status.idle":"2025-03-29T14:56:49.341589Z","shell.execute_reply.started":"2025-03-29T14:56:23.281090Z","shell.execute_reply":"2025-03-29T14:56:49.340830Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":" # 1. Load and preprocess the dataset\ndata_path = \"/kaggle/input/recipedata/recipes.csv\"\ndf = pd.read_csv(data_path)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_end = int(0.8 * len(df))\nval_end = int(0.9 * len(df))\ntrain_df = df.iloc[:train_end]\nval_df = df.iloc[train_end:val_end]\ntest_df = df.iloc[val_end:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:49.342760Z","iopub.execute_input":"2025-03-29T14:56:49.343470Z","iopub.status.idle":"2025-03-29T14:56:50.053873Z","shell.execute_reply.started":"2025-03-29T14:56:49.343434Z","shell.execute_reply":"2025-03-29T14:56:50.053145Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_prompts = [f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"for title, ing in zip(train_df[\"Title\"], train_df[\"Cleaned_Ingredients\"])]\ntrain_targets = [str(instr) for instr in train_df[\"Instructions\"].tolist()]\nval_prompts = [f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"for title, ing in zip(val_df[\"Title\"], val_df[\"Cleaned_Ingredients\"])]\nval_targets = [str(instr) for instr in val_df[\"Instructions\"].tolist()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:50.055549Z","iopub.execute_input":"2025-03-29T14:56:50.055775Z","iopub.status.idle":"2025-03-29T14:56:50.078204Z","shell.execute_reply.started":"2025-03-29T14:56:50.055756Z","shell.execute_reply":"2025-03-29T14:56:50.077246Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df[\"Instructions\"] = train_df[\"Instructions\"].fillna(\"\")\nval_df[\"Instructions\"] = val_df[\"Instructions\"].fillna(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:50.079698Z","iopub.execute_input":"2025-03-29T14:56:50.079995Z","iopub.status.idle":"2025-03-29T14:56:50.098846Z","shell.execute_reply.started":"2025-03-29T14:56:50.079973Z","shell.execute_reply":"2025-03-29T14:56:50.098005Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-eac62ebe452d>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_df[\"Instructions\"] = train_df[\"Instructions\"].fillna(\"\")\n<ipython-input-4-eac62ebe452d>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df[\"Instructions\"] = val_df[\"Instructions\"].fillna(\"\")\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":" # 2. Initialize the tokenizer and model\nmodel_name = \"t5-small\"  # or \"t5-base\" for a slightly larger model\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:50.099612Z","iopub.execute_input":"2025-03-29T14:56:50.099872Z","iopub.status.idle":"2025-03-29T14:56:53.171254Z","shell.execute_reply.started":"2025-03-29T14:56:50.099851Z","shell.execute_reply":"2025-03-29T14:56:53.170346Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9164d1f8284a4c86839057d29d7830ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c473283db9684532be5c8169a2a27782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1c0d95217ab4251ad85f3815684f6b8"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff085b2c8102436da4a69b5abb8152b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417954fe4b8a4103b85d6f771f08716a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7fe6c408806453d87d06b2ccb0bc24c"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"for i, target in enumerate(train_targets):\n    if not isinstance(target, str):\n        print(f\"Non-string element at index {i}: {target} (type: {type(target)})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:53.172229Z","iopub.execute_input":"2025-03-29T14:56:53.172562Z","iopub.status.idle":"2025-03-29T14:56:53.180919Z","shell.execute_reply.started":"2025-03-29T14:56:53.172529Z","shell.execute_reply":"2025-03-29T14:56:53.180091Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":" # 3. Tokenize the prompts and targets\n # We will tokenize such that the model's encoder gets the prompt and decoder learns to generate the target.\n # Use padding and truncation to handle varying lengths.\nmax_input_length = 512   # max tokens for input (adjustable based on dataset)\nmax_target_length = 512  # max tokens for output (adjust as needed)\n # Tokenize training data\n # Tokenize training data\ntrain_encodings = tokenizer(train_prompts, padding=True, truncation=True, max_length=max_input_length)\nwith tokenizer.as_target_tokenizer():\n    train_target_encodings = tokenizer(train_targets, padding=True, truncation=True, max_length=max_target_length)\n\n# Tokenize validation data\nval_encodings = tokenizer(val_prompts, padding=True, truncation=True, max_length=max_input_length)\nwith tokenizer.as_target_tokenizer():\n    val_target_encodings = tokenizer(val_targets, padding=True, truncation=True, max_length=max_target_length)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:56:53.181899Z","iopub.execute_input":"2025-03-29T14:56:53.182107Z","iopub.status.idle":"2025-03-29T14:57:13.051073Z","shell.execute_reply.started":"2025-03-29T14:56:53.182088Z","shell.execute_reply":"2025-03-29T14:57:13.050368Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset\ndf['label'] = df['Image_Name'].apply(lambda x: x.split('_')[0])\n\n# from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        # Ensure every key in encodings returns its idx-th element\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create your dataset instances\ntrain_dataset = MyDataset(train_encodings, train_target_encodings[\"input_ids\"])\nval_dataset = MyDataset(val_encodings, val_target_encodings[\"input_ids\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:57:13.054123Z","iopub.execute_input":"2025-03-29T14:57:13.054378Z","iopub.status.idle":"2025-03-29T14:57:13.065908Z","shell.execute_reply.started":"2025-03-29T14:57:13.054358Z","shell.execute_reply":"2025-03-29T14:57:13.064962Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_dict({\n    \"input_ids\": train_encodings[\"input_ids\"],\n    \"attention_mask\": train_encodings[\"attention_mask\"],\n    \"labels\": train_target_encodings[\"input_ids\"]\n})\n\nval_dataset = Dataset.from_dict({\n    \"input_ids\": val_encodings[\"input_ids\"],\n    \"attention_mask\": val_encodings[\"attention_mask\"],\n    \"labels\": val_target_encodings[\"input_ids\"]\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:57:13.068088Z","iopub.execute_input":"2025-03-29T14:57:13.068448Z","iopub.status.idle":"2025-03-29T14:57:16.483167Z","shell.execute_reply.started":"2025-03-29T14:57:13.068424Z","shell.execute_reply":"2025-03-29T14:57:16.482490Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":" # Note: In the labels, T5 uses -100 internally for padding positions by default \n# (the DataCollator will handle replacing pad tokens with -100 in labels).\n # 4. Set up the Trainer with TrainingArguments\noutput_dir = \"t5_recipe_model\" \ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    disable_tqdm=False,\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",       # evaluate at end of each epoch\n    save_strategy=\"epoch\",             # save model at end of each epoch\n    logging_strategy=\"steps\",\n    logging_steps=50,                  # log training loss every 50 steps\n    save_total_limit=1,                # only keep the best model (1 checkpoint)\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\", # use validation loss to select best model\n    greater_is_better=False,\n    fp16=True,                         # use mixed precision for speed (A100 supports this)\n    report_to=\"none\"                   # no third-party logging (just print to console)\n )\n # Use DataCollatorForSeq2Seq to handle padding of sequences and labels\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n # 5. Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n    # (We don't define compute_metrics here, we'll compute metrics in the evaluation script)\n )\n # 6. Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:57:16.484022Z","iopub.execute_input":"2025-03-29T14:57:16.484333Z","iopub.status.idle":"2025-03-29T15:30:06.633620Z","shell.execute_reply.started":"2025-03-29T14:57:16.484304Z","shell.execute_reply":"2025-03-29T15:30:06.632884Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2025' max='2025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2025/2025 32:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.487300</td>\n      <td>1.297780</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.404700</td>\n      <td>1.239114</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.385800</td>\n      <td>1.224138</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2025, training_loss=1.5173396659191745, metrics={'train_runtime': 1969.2004, 'train_samples_per_second': 16.453, 'train_steps_per_second': 1.028, 'total_flos': 4385074367692800.0, 'train_loss': 1.5173396659191745, 'epoch': 3.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":" # 7. Save the best model and tokenizer\ntrainer.save_model(output_dir)            # this saves the best model (because \nload_best_model_at_end=True\ntokenizer.save_pretrained(output_dir)     # save the tokenizer files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:06.634507Z","iopub.execute_input":"2025-03-29T15:30:06.634850Z","iopub.status.idle":"2025-03-29T15:30:07.208741Z","shell.execute_reply.started":"2025-03-29T15:30:06.634816Z","shell.execute_reply":"2025-03-29T15:30:07.207724Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('t5_recipe_model/tokenizer_config.json',\n 't5_recipe_model/special_tokens_map.json',\n 't5_recipe_model/spiece.model',\n 't5_recipe_model/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":" # 8. Plot training & validation loss curves\n # Extract logged history of losses\nlogs = trainer.state.log_history\ntrain_losses = [entry[\"loss\"] for entry in logs if \"loss\" in entry]\neval_losses = [entry[\"eval_loss\"] for entry in logs if \"eval_loss\" in entry]\nplt.figure()\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(eval_losses, label=\"Validation Loss\")\nplt.xlabel(\"Logging Step or Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"T5 Fine-Tuning Loss\")\nplt.legend()\nplt.savefig(os.path.join(output_dir, \"loss_curve.png\"))\nplt.close()\nprint(\"Training complete. Best model saved to\", output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:07.209563Z","iopub.execute_input":"2025-03-29T15:30:07.209859Z","iopub.status.idle":"2025-03-29T15:30:07.505926Z","shell.execute_reply.started":"2025-03-29T15:30:07.209827Z","shell.execute_reply":"2025-03-29T15:30:07.504958Z"}},"outputs":[{"name":"stdout","text":"Training complete. Best model saved to t5_recipe_model\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:07.506657Z","iopub.execute_input":"2025-03-29T15:30:07.506864Z","iopub.status.idle":"2025-03-29T15:30:08.046059Z","shell.execute_reply.started":"2025-03-29T15:30:07.506845Z","shell.execute_reply":"2025-03-29T15:30:08.044982Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:08.047036Z","iopub.execute_input":"2025-03-29T15:30:08.047325Z","iopub.status.idle":"2025-03-29T15:30:14.255659Z","shell.execute_reply.started":"2025-03-29T15:30:08.047304Z","shell.execute_reply":"2025-03-29T15:30:14.254756Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport evaluate\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:14.256640Z","iopub.execute_input":"2025-03-29T15:30:14.256957Z","iopub.status.idle":"2025-03-29T15:30:14.452504Z","shell.execute_reply.started":"2025-03-29T15:30:14.256918Z","shell.execute_reply":"2025-03-29T15:30:14.451796Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":" # 1. Load the test data (same splitting logic to get test_df as used in training)\ndata_path = \"//kaggle/input/recipedata/recipes.csv\"\ndf = pd.read_csv(data_path)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\nval_end = int(0.9 * len(df))\ntest_df = df.iloc[val_end:]  # last 10% as test\ntest_prompts = [\n   f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"\n   for title, ing in zip(test_df[\"Title\"], test_df[\"Cleaned_Ingredients\"])]\ntest_references = test_df[\"Instructions\"].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:14.453550Z","iopub.execute_input":"2025-03-29T15:30:14.453897Z","iopub.status.idle":"2025-03-29T15:30:14.827956Z","shell.execute_reply.started":"2025-03-29T15:30:14.453865Z","shell.execute_reply":"2025-03-29T15:30:14.827208Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":" # 2. Load the fine-tuned model and tokenizer\nmodel_dir = \"/kaggle/working/t5_recipe_model\"  # path where the fine-tuned model is saved\ntokenizer = T5Tokenizer.from_pretrained(model_dir)\nmodel = T5ForConditionalGeneration.from_pretrained(model_dir)\nmodel.eval()\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:14.828711Z","iopub.execute_input":"2025-03-29T15:30:14.828926Z","iopub.status.idle":"2025-03-29T15:30:15.351497Z","shell.execute_reply.started":"2025-03-29T15:30:14.828907Z","shell.execute_reply":"2025-03-29T15:30:15.350712Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# !pip install meteor_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:30:05.212837Z","iopub.execute_input":"2025-03-29T16:30:05.213155Z","iopub.status.idle":"2025-03-29T16:30:05.217586Z","shell.execute_reply.started":"2025-03-29T16:30:05.213125Z","shell.execute_reply":"2025-03-29T16:30:05.216597Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":" # 3. Generate predictions for the test set\npredictions = []\nfor prompt in test_prompts:\n# Tokenize the input prompt and generate output\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n    outputs = model.generate(inputs, max_length=300, num_beams=4, early_stopping=True)\n    # Decode the generated sequence to text\n    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(pred_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:30:15.352345Z","iopub.execute_input":"2025-03-29T15:30:15.352627Z","iopub.status.idle":"2025-03-29T16:22:43.429664Z","shell.execute_reply.started":"2025-03-29T15:30:15.352592Z","shell.execute_reply":"2025-03-29T16:22:43.428655Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:22:43.430696Z","iopub.execute_input":"2025-03-29T16:22:43.430991Z","iopub.status.idle":"2025-03-29T16:22:49.091017Z","shell.execute_reply.started":"2025-03-29T16:22:43.430962Z","shell.execute_reply":"2025-03-29T16:22:49.089879Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=2d7db4fbe313ab3172f5c799e06c34f6440f6820d0084d30f62cb9bb479c6406\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 4. Compute EVALUATION Metrics\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\n# meteor = evaluate.load(\"meteor\")\n# BLEU expects a list of references for each prediction (e.g., list of lists)\nbleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in test_references])[\"bleu\"]\nrouge_score = rouge.compute(predictions=predictions, references=test_references)\n# meteor_score = meteor.compute(predictions=predictions, references=test_references)[\"meteor\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:30:55.471675Z","iopub.execute_input":"2025-03-29T16:30:55.471982Z","iopub.status.idle":"2025-03-29T16:31:20.513624Z","shell.execute_reply.started":"2025-03-29T16:30:55.471958Z","shell.execute_reply":"2025-03-29T16:31:20.512902Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":" # We will use ROUGE-L F1 score as our ROUGE-L metric:\nrougeL = rouge_score[\"rougeL\"]  # this is typically the F1 score for ROUGE-L\n # (The rouge metric returns several values; we take 'rougeL' which is the F1 measure of longest common subsequence.)\n # F1-score: define as average token overlap F1 (precision/recall over words)\ndef token_overlap_f1(pred, ref):\n    pred_tokens = pred.split()\n    ref_tokens = ref.split()\n    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n        return 0.0\n    common = set(pred_tokens) & set(ref_tokens)\n    prec = len(common) / len(set(pred_tokens))\n    rec = len(common) / len(set(ref_tokens))\n    if prec + rec == 0:\n        return 0.0\n    return 2 * prec * rec / (prec + rec)\nf1_scores = [token_overlap_f1(p, r) for p, r in zip(predictions, test_references)]\navg_f1 = sum(f1_scores) / len(f1_scores)\nprint(f\"BLEU: {bleu_score:.4f}\")\nprint(f\"ROUGE-L: {rougeL:.4f}\")\n# print(f\"METEOR: {meteor_score:.4f}\")\nprint(f\"F1-score: {avg_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:31:20.514725Z","iopub.execute_input":"2025-03-29T16:31:20.514973Z","iopub.status.idle":"2025-03-29T16:31:20.605635Z","shell.execute_reply.started":"2025-03-29T16:31:20.514953Z","shell.execute_reply":"2025-03-29T16:31:20.604727Z"}},"outputs":[{"name":"stdout","text":"BLEU: 0.0600\nROUGE-L: 0.2076\nF1-score: 0.2462\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":" # 5. Print qualitative examples\nprint(\"\\nSample Predictions:\")\nfor idx in random.sample(range(len(test_prompts)), 3):\n    print(f\"Prompt: {test_prompts[idx]}\")\n    print(\"-\" * 50)\n    print(f\"Reference: {test_references[idx]}\")\n    print(\"-\" * 50)\n    print(f\"Generated: {predictions[idx]}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:31:20.606881Z","iopub.execute_input":"2025-03-29T16:31:20.607163Z","iopub.status.idle":"2025-03-29T16:31:20.613648Z","shell.execute_reply.started":"2025-03-29T16:31:20.607140Z","shell.execute_reply":"2025-03-29T16:31:20.612915Z"}},"outputs":[{"name":"stdout","text":"\nSample Predictions:\nPrompt: Title: Spaghetti with No-Cook Tomato Sauce and Hazelnuts\nIngredients: ['1/2 cup blanched hazelnuts', '1 pound cherry tomatoes, halved', '1 teaspoon kosher salt, plus more', '12 ounces spaghetti or linguine', '1 beefsteak tomato, chopped', '2 garlic cloves, crushed', '1 teaspoon crushed red pepper flakes', '1 cup basil leaves, divided', '2 small zucchini (about 8 ounces), coarsely grated', '1/4 cup olive oil, plus more for drizzling', 'Freshly ground black pepper', '1 ounce ricotta salata (salted dry ricotta)', 'shaved']\nInstructions:\nReference: Preheat oven to 350°F. Toast hazelnuts on a rimmed baking sheet, tossing once, until golden brown, 8–10 minutes. Let cool, then coarsely chop.\nPlace cherry tomatoes in a large bowl; season with salt.\nCook spaghetti in a large pot of boiling salted water, stirring occasionally, until al dente. Drain pasta, reserving 1/4 cup pasta cooking liquid.\nMeanwhile, puree beefsteak tomato, garlic, red pepper flakes, 1/2 cup basil, 3 tablespoons chopped hazelnuts, and 1 teaspoon salt in a food processor until smooth; add to bowl with salted cherry tomatoes. Add zucchini, spaghetti, pasta cooking liquid, 1/4 cup oil, and remaining 1/2 cup basil leaves and toss to combine; season with salt and pepper.\nDivide pasta among bowls, drizzle with more oil, and top with ricotta salata and remaining hazelnuts.\nGenerated: Preheat oven to 350°F. Add hazelnuts, cherry tomatoes, salt, and more. Add spaghetti or linguine, beefsteak tomato, garlic, red pepper flakes, basil, zucchini, olive oil, and ricotta salata, and shaved.\n--------------------------------------------------\nPrompt: Title: Grilled Shrimp and Corn Salad\nIngredients: ['1 pound large peeled, deveined shrimp', '1 ounce tequila', 'Juice of 2 limes', '3 ears of corn, husks removed', '2 heads romaine lettuce, chopped', '1/2 cup black beans, drained and rinsed', '1/2 cup cherry tomatoes, halved', '1 avocado, sliced', '1/4 cup your favorite salad dressing', 'Chopped fresh cilantro, for garnish', 'Sliced lime, for garnish', 'Tortilla chips (optional)']\nInstructions:\nReference: Thread the shrimp on skewers and place in a baking dish. Coat with tequila and lime juice and marinate 15 minutes. Oil and preheat a grill or grill pan to medium-high heat. Brush the corn with olive oil and sprinkle with salt and pepper. Place the shrimp and corn on the grill and grill about 3 minutes per side until cooked through. Cool slightly. Remove the shrimp from skewers and slice the kernels off the cobs.\nIn a large bowl, toss the lettuce with the beans, tomatoes, avocado, and dressing. Arrange grilled shrimp and corn on top and garnish with cilantro and lime. Serve with tortilla chips, if desired.\nGenerated: Combine shrimp, tequila, limes, corn, romaine lettuce, beans, cherry tomatoes, avocado, salad dressing, cilantro, and cilantro in a large bowl. Season with salt and pepper.\n--------------------------------------------------\nPrompt: Title: Maple-Gingerbread Layer Cake with Salted Maple-Caramel Sauce\nIngredients: ['3/4 cup pecan halves, toasted', '1/4 cup pure maple syrup (preferably Grade B)', 'Coarse kosher salt', '2 1/2 cups all purpose flour', '1 1/2 teaspoons Chinese five-spice powder', '1 teaspoon baking soda', '1/2 teaspoon coarse kosher salt', '1/3 cup chopped crystallized ginger (1 1/2 to 2 ounces)', '1 cup maple sugar', '3/4 cup (1 1/2 sticks) unsalted butter, room temperature', '2 large eggs', '3/4 cup hot water', '2/3 cup mild-flavored (light) molasses', '1 1/3 cups chilled crème fraîche', '1 1/3 cups chilled heavy whipping cream', '1/2 cup maple sugar', '6 tablespoons powdered sugar', 'Salted Maple-Caramel Sauce', 'Ingredient info: Chinese five-spice powder-a spice blend that usually contains ground fennel seeds', 'Sichuan peppercorns', 'cinnamon', 'star anise', 'and cloves-is available in the spice section of most supermarkets. Crème fraîche is sold at most supermarkets and at specialty foods stores.']\nInstructions:\nReference: Place large piece of foil on work surface. Combine nuts and maple syrup in heavy medium skillet (do not use nonstick) over medium-high heat and bring to boil, tossing to coat. Cook until syrup is dark amber and almost cooked away and thickly coats nuts, tossing often, 3 to 3 1/2 minutes. Scrape nuts onto foil. Working quickly with 2 forks, separate nuts. Sprinkle with coarse salt. Cool until coating is crisp and hard, about 1 hour. DO AHEAD: Pecans can be made 1 day ahead. Store airtight at room temperature.\nPreheat oven to 350°F. Butter and flour two 9-inch-diameter cake pans with 1 1/2-inch-high sides. Combine first 4 ingredients in processor; add ginger. Blend until ginger is finely ground, about 1 minute. Using mixer, beat maple sugar and butter in large bowl until fluffy. Beat in eggs 1 at a time (batter may look curdled). Stir 3/4 cup hot water and molasses in small bowl. Beat dry ingredients into butter mixture in 4 additions alternately with molasses mixture in 3 additions.\nDivide batter between prepared pans (about 21/2 cups each). Bake until tester inserted into center comes out clean, 30 to 32 minutes. Cool cakes in pans on racks.\nCombine crème fraîche, cream, and both sugars in large bowl. Using electric mixer, beat until very thick and stiff.\nCut around pan sides to loosen cake layers; turn out onto racks. Place 1 cake layer on platter. Spread with 1 1/3 cups frosting. Drizzle with 3 tablespoons caramel sauce. Top with second cake layer. Spread remaining frosting smoothly over top and sides of cake. Drizzle top of cake with 3 tablespoons sauce. Cover with cake dome; chill at least 1 hour. DO AHEAD: Can be made 1 day ahead; keep chilled. Let stand at room temperature 30 minutes before continuing.\nCut pecans into pieces or leave whole. Press pecans into frosting on sides of cake. Cut cake into wedges. Spoon sauce over.\nGenerated: Preheat oven to 375°F. Place pecan halves, maple syrup, and salt in a large bowl. Add flour, five-spice powder, baking soda, and salt in a large bowl. Add ginger, maple sugar, butter, molasses, cream, maple sugar, and powdered sugar in a large bowl. Add crème fraîche, cream, maple sugar, and powdered sugar in a large bowl. Add Crème fraîche to a large pot of boiling water and cook, stirring occasionally, until golden brown, about 5 minutes. Add molasses, molasses, and molasses in a large saucepan over medium-high heat. Add fennel seeds, peppercorns, and cinnamon, and cloves. Add molasses, molasses, cream, and sugar in a medium bowl to a large bowl. Add cream, maple syrup, and powdered sugar and cook, stirring occasionally, until golden brown, about 5 minutes. Add molasses and cook, stirring occasionally, until golden brown, about 5 minutes. Add molasses, cream, maple syrup, and molasses and cook, stirring occasionally, until golden brown, about 5 minutes. Serve with Crème fraîche.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}