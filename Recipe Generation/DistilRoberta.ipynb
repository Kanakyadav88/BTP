{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# distilroberta_trainer.py\n import pandas as pd\n import torch\n from transformers import AutoTokenizer, EncoderDecoderModel, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n import matplotlib.pyplot as plt\n import os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_path = \"recipes.csv\"\ndf = pd.read_csv(data_path)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ntrain_end = int(0.8 * len(df))\nval_end = int(0.9 * len(df))\ntrain_df = df.iloc[:train_end]\nval_df = df.iloc[train_end:val_end]\ntest_df = df.iloc[val_end:]\ntrain_prompts = [f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"for title, ing in zip(train_df[\"Title\"], train_df[\"Cleaned_Ingredients\"])]\ntrain_targets = train_df[\"Instructions\"].tolist()\nval_prompts = [\n    f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"\n    for title, ing in zip(val_df[\"Title\"], val_df[\"Cleaned_Ingredients\"])\n]\nval_targets = val_df[\"Instructions\"].tolist()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 2. Initialize tokenizer and EncoderDecoderModel for DistilRoBERTa\n encoder_name = \"distilroberta-base\"\n decoder_name = \"distilroberta-base\"\n tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n model = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_name, decoder_name)\n # Set special tokens:\n # RoBERTa uses <s> as bos, </s> as eos, and has <pad>.\n model.config.decoder_start_token_id = tokenizer.bos_token_id   # start decoding with <s>\n model.config.eos_token_id = tokenizer.eos_token_id             # end of sequence </s>\n model.config.pad_token_id = tokenizer.pad_token_id             # pad token\n model.config.vocab_size = model.config.decoder.vocab_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 3. Tokenization\n max_input_length = 512\n max_target_length = 512\n train_encodings = tokenizer(train_prompts, padding=True, truncation=True, max_length=max_input_length)\n with tokenizer.as_target_tokenizer():\n    train_target_encodings = tokenizer(train_targets, padding=True, truncation=True, max_length=max_target_length)\n val_encodings = tokenizer(val_prompts, padding=True, truncation=True, max_length=max_input_length)\n\n with tokenizer.as_target_tokenizer():\n    val_target_encodings = tokenizer(val_targets, padding=True, truncation=True, max_length=max_target_length)\n train_dataset = {\n    \"input_ids\": train_encodings[\"input_ids\"],\n    \"attention_mask\": train_encodings[\"attention_mask\"],\n    \"labels\": train_target_encodings[\"input_ids\"]\n }\n val_dataset = {\n    \"input_ids\": val_encodings[\"input_ids\"],\n    \"attention_mask\": val_encodings[\"attention_mask\"],\n    \"labels\": val_target_encodings[\"input_ids\"]\n }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 4. Training setup\n output_dir = \"distilroberta_recipe_model\"\n training_args = TrainingArguments(\n    output_dir=output_dir,\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    report_to=\"none\"\n )\n data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator\n )\n\n # 5. Train the model\n trainer.train()\n # 6. Save model and tokenizer\n trainer.save_model(output_dir)\n tokenizer.save_pretrained(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 7. Plot loss curves\n logs = trainer.state.log_history\n train_losses = [entry.get(\"loss\") for entry in logs if \"loss\" in entry]\n eval_losses = [entry.get(\"eval_loss\") for entry in logs if \"eval_loss\" in entry]\n plt.figure()\n plt.plot(train_losses, label=\"Training Loss\")\n plt.plot(eval_losses, label=\"Validation Loss\")\n plt.xlabel(\"Logging Step or Epoch\")\n plt.ylabel(\"Loss\")\n plt.title(\"DistilRoBERTa2DistilRoBERTa Fine-Tuning Loss\")\n plt.legend()\n plt.savefig(os.path.join(output_dir, \"loss_curve.png\"))\n plt.close()\n print(\"DistilRoBERTa encoder-decoder fine-tuning complete. Model saved to\", \noutput_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# distilroberta_evaluator.py\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, EncoderDecoderModel\nimport evaluate\nimport random","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load test data\ndata_path = \"recipes.csv\"\ndf = pd.read_csv(data_path)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\nval_end = int(0.9 * len(df))\ntest_df = df.iloc[val_end:]\ntest_prompts = [\n    f\"Title: {title}\\nIngredients: {ing}\\nInstructions:\"\n    for title, ing in zip(test_df[\"Title\"], test_df[\"Cleaned_Ingredients\"])\n]\ntest_references = test_df[\"Instructions\"].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Load the model and tokenizer\nmodel_dir = \"distilroberta_recipe_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = EncoderDecoderModel.from_pretrained(model_dir)\nmodel.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Generate outputs for test prompts\npredictions = []\nfor prompt in test_prompts:\n    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n    outputs = model.generate(input_ids, max_length=300, num_beams=4, early_stopping=True)\n    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predictions.append(pred_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 4. Compute evaluation metrics\nbleu = evaluate.load(\"bleu\")\nrouge = evaluate.load(\"rouge\")\n# meteor = evaluate.load(\"meteor\")\n\nbleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in test_references])[\"bleu\"]\nrouge_result = rouge.compute(predictions=predictions, references=test_references)\n# meteor_score = meteor.compute(predictions=predictions, references=test_references)[\"meteor\"]\nrougeL = rouge_result[\"rougeL\"]\ndef token_overlap_f1(pred, ref):\n    pred_tokens = pred.split()\n    ref_tokens = ref.split()\n    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n        return 0.0\n    common = set(pred_tokens) & set(ref_tokens)\n    prec = len(common) / len(set(pred_tokens))\n    rec = len(common) / len(set(ref_tokens))\n    if prec + rec == 0:\n        return 0.0\n    return 2 * prec * rec / (prec + rec)\navg_f1 = sum(token_overlap_f1(p, r) for p, r in zip(predictions, test_references)) / len(predictions)\nprint(f\"BLEU: {bleu_score:.4f}\")\nprint(f\"ROUGE-L: {rougeL:.4f}\")\n# print(f\"METEOR: {meteor_score:.4f}\")\nprint(f\"F1-score: {avg_f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # 5. Show some example predictions\nprint(\"\\nSample Predictions:\")\nfor idx in random.sample(range(len(test_prompts)), 3):\n    print(f\"Prompt: {test_prompts[idx]}\")\n    print(\"-\" * 50)\n    print(f\"Reference: {test_references[idx]}\")\n    print(\"-\" * 50)\n    print(f\"Generated: {predictions[idx]}\")\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}